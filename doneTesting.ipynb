{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886035f3-34d5-4ec1-b9f4-96d608b32067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ngrant/.local/lib/python3.11/site-packages (4.57.3)\n",
      "Requirement already satisfied: diffusers[torch] in /home/ngrant/.local/lib/python3.11/site-packages (0.36.0)\n",
      "Requirement already satisfied: importlib_metadata in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (7.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (3.13.1)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /home/ngrant/.local/lib/python3.11/site-packages (from diffusers[torch]) (0.36.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (0.4.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (10.4.0)\n",
      "Requirement already satisfied: torch>=1.4 in /home/ngrant/.local/lib/python3.11/site-packages (from diffusers[torch]) (2.9.1)\n",
      "Requirement already satisfied: accelerate>=0.31.0 in /opt/conda/lib/python3.11/site-packages (from diffusers[torch]) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ngrant/.local/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.31.0->diffusers[torch]) (5.9.8)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers[torch]) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers[torch]) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers[torch]) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers[torch]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->diffusers[torch]) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers[torch]) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ngrant/.local/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers[torch]) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/ngrant/.local/lib/python3.11/site-packages (from torch>=1.4->diffusers[torch]) (3.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata->diffusers[torch]) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers[torch]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers[torch]) (2.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.4->diffusers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.4->diffusers[torch]) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "print(sys.executable)  # just to see which Python the notebook is using\n",
    "\n",
    "# Install into THIS Python, not some other one\n",
    "!{sys.executable} -m pip install --upgrade diffusers[torch] transformers\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bb1bee-0dd7-4deb-baeb-a59d379068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d190bd-a475-4acf-9b58-3bfd660b358a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57bc5070efe4defb445acb4d74c4485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1.  Load unCLIP – vision side only (projection_dim = 1024)   ─\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "    \"sd2-community/stable-diffusion-2-1-unclip\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "vision_encoder = pipe.image_encoder         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c9810e-9762-4032-b726-0862784c7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openclip_repo = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"     # projection_dim = 1024 :contentReference[oaicite:0]{index=0}\n",
    "tokenizer = CLIPTokenizer.from_pretrained(openclip_repo)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "    openclip_repo,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# optional: stuff them into the pipe so `pipe.tokenizer` etc. work\n",
    "pipe.tokenizer, pipe.text_encoder = tokenizer, text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7366fa-99da-4b88-86bf-f77080f06ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_images(paths, batch_size=8):\n",
    "    \"\"\"Return (N,1024) image embeddings\"\"\"\n",
    "    out, fe, enc = [], pipe.feature_extractor, pipe.image_encoder\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        print(f\"Batch {i}/{len(paths)}\")\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in paths[i:i + batch_size]]\n",
    "        px   = fe(imgs, return_tensors=\"pt\").pixel_values.to(enc.device, enc.dtype)\n",
    "        with torch.no_grad():\n",
    "            v = enc(px)[0]                              # (B,1024)\n",
    "        out.append(v)\n",
    "    return torch.cat(out)  # (N,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed115ff-a5c3-45dd-a754-cf20502c6f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mammals -> 80 images\n",
      "sea_animals -> 71 images\n",
      "reptiles -> 86 images\n",
      "bugs -> 88 images\n",
      "birds -> 97 images\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Deal with file structure and get working paths\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "root = \"THINGS_animalgroups\"\n",
    "\n",
    "# collect all jpgs and keep their group (top-level folder)\n",
    "groupedimages = {}\n",
    "\n",
    "for group in os.listdir(root):\n",
    "    group_dir = os.path.join(root, group)\n",
    "    \n",
    "\n",
    "    groupedimages[group] = []\n",
    "\n",
    "    #go into animal name from category\n",
    "    for animal in os.listdir(group_dir):\n",
    "        animal_dir = os.path.join(group_dir, animal)\n",
    "        if not os.path.isdir(animal_dir):\n",
    "            continue\n",
    "\n",
    "        # animal images inside animal files\n",
    "        for fname in os.listdir(animal_dir):\n",
    "            if fname.lower().endswith(\".jpg\"):\n",
    "                full_path = os.path.join(animal_dir, fname)\n",
    "                groupedimages[group].append(full_path)\n",
    "\n",
    "# test\n",
    "for g, imgs in groupedimages.items():\n",
    "    print(g, \"->\", len(imgs), \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78fec6d4-444c-4b72-80d3-fe5f1c06c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'birds': 0, 'bugs': 1, 'mammals': 2, 'reptiles': 3, 'sea_animals': 4}\n",
      "Total training images: 422\n"
     ]
    }
   ],
   "source": [
    "#map each group\n",
    "groups = sorted(groupedimages.keys())   # deterministic order\n",
    "group_to_idx = {g: i for i, g in enumerate(groups)}\n",
    "print(\"Label mapping:\", group_to_idx)\n",
    "\n",
    "all_paths = []\n",
    "all_labels = []\n",
    "\n",
    "for group, paths in groupedimages.items():\n",
    "    for p in paths:\n",
    "        all_paths.append(p)\n",
    "        all_labels.append(group_to_idx[group])\n",
    "\n",
    "all_labels = torch.tensor(all_labels, dtype=torch.long)\n",
    "print(\"Total training images:\", len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbc09d8-288a-468f-8e31-550e8cbae705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/422\n",
      "Batch 8/422\n",
      "Batch 16/422\n",
      "Batch 24/422\n",
      "Batch 32/422\n",
      "Batch 40/422\n",
      "Batch 48/422\n",
      "Batch 56/422\n",
      "Batch 64/422\n",
      "Batch 72/422\n",
      "Batch 80/422\n",
      "Batch 88/422\n",
      "Batch 96/422\n",
      "Batch 104/422\n",
      "Batch 112/422\n",
      "Batch 120/422\n",
      "Batch 128/422\n",
      "Batch 136/422\n",
      "Batch 144/422\n",
      "Batch 152/422\n",
      "Batch 160/422\n",
      "Batch 168/422\n",
      "Batch 176/422\n",
      "Batch 184/422\n",
      "Batch 192/422\n",
      "Batch 200/422\n",
      "Batch 208/422\n",
      "Batch 216/422\n",
      "Batch 224/422\n",
      "Batch 232/422\n",
      "Batch 240/422\n",
      "Batch 248/422\n",
      "Batch 256/422\n",
      "Batch 264/422\n",
      "Batch 272/422\n",
      "Batch 280/422\n",
      "Batch 288/422\n",
      "Batch 296/422\n",
      "Batch 304/422\n",
      "Batch 312/422\n",
      "Batch 320/422\n",
      "Batch 328/422\n",
      "Batch 336/422\n",
      "Batch 344/422\n",
      "Batch 352/422\n",
      "Batch 360/422\n",
      "Batch 368/422\n",
      "Batch 376/422\n",
      "Batch 384/422\n",
      "Batch 392/422\n",
      "Batch 400/422\n",
      "Batch 408/422\n",
      "Batch 416/422\n",
      "Embedding tensor: torch.Size([422, 1024])\n"
     ]
    }
   ],
   "source": [
    "#embedding using given image embedding\n",
    "with torch.no_grad():\n",
    "    img_feats = embed_images(all_paths) # (N, 1024)\n",
    "    img_feats = img_feats.to(torch.float32)\n",
    "    img_feats = F.normalize(img_feats, dim=-1)  # same normalization as notebook\n",
    "\n",
    "print(\"Embedding tensor:\", img_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046bc735-9cef-4f47-bee2-548e0ace5377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes: 5\n",
      "embedding dim: 1024\n",
      "Epoch   2/300 | loss = 1.4272 | train acc = 0.929\n",
      "Epoch  50/300 | loss = 0.1247 | train acc = 0.998\n",
      "Epoch 100/300 | loss = 0.0498 | train acc = 1.000\n",
      "Epoch 150/300 | loss = 0.0344 | train acc = 1.000\n",
      "Epoch 200/300 | loss = 0.0311 | train acc = 1.000\n",
      "Epoch 250/300 | loss = 0.0301 | train acc = 1.000\n",
      "Epoch 300/300 | loss = 0.0299 | train acc = 1.000\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(groups)\n",
    "embed_dim = img_feats.shape[1]\n",
    "\n",
    "print(\"num classes:\", num_classes)\n",
    "print(\"embedding dim:\", embed_dim)\n",
    "\n",
    "train_ds = TensorDataset(img_feats, all_labels.to(device))\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "# basic vlassifier\n",
    "classifier = nn.Linear(embed_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "epochs = 300  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for feats_batch, labels_batch in train_loader:\n",
    "        feats_batch = feats_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        logits = classifier(feats_batch)\n",
    "        loss = criterion(logits, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_ds)\n",
    "\n",
    "    #monitor training accuracy\n",
    "    if (epoch + 1) % 50 == 0 or epoch == 1:\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_all = classifier(img_feats.to(device))\n",
    "            preds = logits_all.argmax(dim=-1).cpu()\n",
    "            acc = (preds == all_labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | loss = {avg_loss:.4f} | train acc = {acc:.3f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b30efc9-655f-41ad-9215-fa3ce98e330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map class indices back to group names\n",
    "idx_to_group = {i: g for g, i in group_to_idx.items()}\n",
    "\n",
    "def classify_new_image(img_path, topk=None):\n",
    "    \n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Get CLIP embedding (same as training)\n",
    "        feat = embed_images([img_path])              # (1, 1024) float16\n",
    "        feat = feat.to(torch.float32)                # match classifier dtype\n",
    "        feat = F.normalize(feat, dim=-1)             # same normalization as training\n",
    "\n",
    "        # 2. Run through trained classifier\n",
    "        logits = classifier(feat.to(next(classifier.parameters()).device))\n",
    "        probs = logits.softmax(dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    pred_idx = probs.argmax()\n",
    "    pred_group = idx_to_group[pred_idx]\n",
    "\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Predicted group: {pred_group}\")\n",
    "    print(\"Class probabilities:\")\n",
    "    for i, g in enumerate(groups):\n",
    "        print(f\"  {g:>12}: {probs[i]:.3f}\")\n",
    "\n",
    "    if topk is not None:\n",
    "        # return top-k labels + probs as a small list for analysis\n",
    "        topk_idx = probs.argsort()[::-1][:topk]\n",
    "        topk_labels = [idx_to_group[i] for i in topk_idx]\n",
    "        topk_probs = probs[topk_idx]\n",
    "        return pred_group, probs, topk_labels, topk_probs\n",
    "\n",
    "    return pred_group, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f121c522-82af-42ef-abad-88df07f68f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/1\n",
      "Image: THINGS_animals/whale/whale_01b.jpg\n",
      "Predicted group: mammals\n",
      "Class probabilities:\n",
      "         birds: 0.003\n",
      "          bugs: 0.005\n",
      "       mammals: 0.975\n",
      "      reptiles: 0.004\n",
      "   sea_animals: 0.012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mammals',\n",
       " array([0.00271764, 0.0054677 , 0.9750072 , 0.00446913, 0.01233828],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_new_image('THINGS_animals/whale/whale_01b.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7ac38-c577-413d-9f5f-85eb26b95c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
