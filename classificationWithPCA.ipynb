{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886035f3-34d5-4ec1-b9f4-96d608b32067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "print(sys.executable)  # just to see which Python the notebook is using\n",
    "\n",
    "# Install into THIS Python, not some other one\n",
    "!{sys.executable} -m pip install --upgrade diffusers[torch] transformers\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bb1bee-0dd7-4deb-baeb-a59d379068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d190bd-a475-4acf-9b58-3bfd660b358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1.  Load unCLIP – vision side only (projection_dim = 1024)   ─\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "    \"sd2-community/stable-diffusion-2-1-unclip\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "vision_encoder = pipe.image_encoder         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c9810e-9762-4032-b726-0862784c7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openclip_repo = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"     # projection_dim = 1024 :contentReference[oaicite:0]{index=0}\n",
    "tokenizer = CLIPTokenizer.from_pretrained(openclip_repo)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "    openclip_repo,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# optional: stuff them into the pipe so `pipe.tokenizer` etc. work\n",
    "pipe.tokenizer, pipe.text_encoder = tokenizer, text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7366fa-99da-4b88-86bf-f77080f06ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_images(paths, batch_size=8):\n",
    "    \"\"\"Return (N,1024) image embeddings\"\"\"\n",
    "    out, fe, enc = [], pipe.feature_extractor, pipe.image_encoder\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        print(f\"Batch {i}/{len(paths)}\")\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in paths[i:i + batch_size]]\n",
    "        px   = fe(imgs, return_tensors=\"pt\").pixel_values.to(enc.device, enc.dtype)\n",
    "        with torch.no_grad():\n",
    "            v = enc(px)[0]                              # (B,1024)\n",
    "        out.append(v)\n",
    "    return torch.cat(out)  # (N,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed115ff-a5c3-45dd-a754-cf20502c6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Deal with file structure and get working paths\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "root = \"THINGS_animalgroups\"\n",
    "\n",
    "# collect all jpgs and keep their group (top-level folder)\n",
    "groupedimages = {}\n",
    "\n",
    "for group in os.listdir(root):\n",
    "    group_dir = os.path.join(root, group)\n",
    "    groupedimages[group] = []\n",
    "\n",
    "    #go into animal name from category\n",
    "    for animal in os.listdir(group_dir):\n",
    "        animal_dir = os.path.join(group_dir, animal)\n",
    "        if not os.path.isdir(animal_dir):\n",
    "            continue\n",
    "\n",
    "        # animal images inside animal files\n",
    "        for fname in os.listdir(animal_dir):\n",
    "            if fname.lower().endswith(\".jpg\"):\n",
    "                full_path = os.path.join(animal_dir, fname)\n",
    "                groupedimages[group].append(full_path)\n",
    "# test\n",
    "for g, imgs in groupedimages.items():\n",
    "    print(g, \"->\", len(imgs), \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fec6d4-444c-4b72-80d3-fe5f1c06c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map each group\n",
    "groups = sorted(groupedimages.keys())   # deterministic order\n",
    "group_to_idx = {g: i for i, g in enumerate(groups)}\n",
    "print(\"Label mapping:\", group_to_idx)\n",
    "\n",
    "all_paths = []\n",
    "all_labels = []\n",
    "\n",
    "for group, paths in groupedimages.items():\n",
    "    for p in paths:\n",
    "        all_paths.append(p)\n",
    "        all_labels.append(group_to_idx[group])\n",
    "\n",
    "all_labels = torch.tensor(all_labels, dtype=torch.long)\n",
    "print(\"Total training images:\", len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc09d8-288a-468f-8e31-550e8cbae705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding using given image embedding\n",
    "with torch.no_grad():\n",
    "    img_feats = embed_images(all_paths) # (N, 1024)\n",
    "    img_feats = img_feats.to(torch.float32)\n",
    "    img_feats = F.normalize(img_feats, dim=-1)  # same normalization as notebook\n",
    "\n",
    "\n",
    "print(\"Embedding tensor:\", img_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bc735-9cef-4f47-bee2-548e0ace5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(groups)\n",
    "embed_dim = img_feats.shape[1]\n",
    "\n",
    "print(\"num classes:\", num_classes)\n",
    "print(\"embedding dim:\", embed_dim)\n",
    "\n",
    "train_ds = TensorDataset(img_feats, all_labels.to(device))\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "# basic vlassifier\n",
    "classifier = nn.Linear(embed_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "epochs = 100  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for feats_batch, labels_batch in train_loader:\n",
    "        feats_batch = feats_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        logits = classifier(feats_batch)\n",
    "        loss = criterion(logits, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_ds)\n",
    "\n",
    "    #monitor training accuracy\n",
    "    if (epoch + 1) % 50 == 0 or epoch == 1:\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_all = classifier(img_feats.to(device))\n",
    "            preds = logits_all.argmax(dim=-1).cpu()\n",
    "            acc = (preds == all_labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | loss = {avg_loss:.4f} | train acc = {acc:.3f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b30efc9-655f-41ad-9215-fa3ce98e330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map class indices back to group names\n",
    "idx_to_group = {i: g for g, i in group_to_idx.items()}\n",
    "classificationResults = []\n",
    "\n",
    "def classifyNewImage(img_path, topk=None):\n",
    "    \n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        feat = embed_images([img_path])\n",
    "        feat = feat.to(torch.float32)\n",
    "        feat = F.normalize(feat, dim=-1)\n",
    "        \n",
    "        logits = classifier(feat.to(next(classifier.parameters()).device))\n",
    "        probs = logits.softmax(dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    pred_idx = probs.argmax()\n",
    "    pred_group = idx_to_group[pred_idx]\n",
    "    pred_conf = float(probs[pred_idx])\n",
    "    pred_conf_pct = pred_conf * 100.0\n",
    "    \n",
    "    #to create data table display\n",
    "    row = {\n",
    "        \"image\": os.path.basename(img_path),\n",
    "        \"path\": img_path,\n",
    "        \"pred_group\": pred_group,\n",
    "        \"confidence_pct\": pred_conf_pct,\n",
    "    }\n",
    "\n",
    "    classificationResults.append(row)\n",
    "\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Predicted group: {pred_group} ({pred_conf_pct:.1f}% confidence)\")\n",
    "    print(\"Class probabilities:\")\n",
    "    for i, g in enumerate(groups):\n",
    "        print(f\"  {g:>12}: {probs[i]*100:.1f}%\")\n",
    "\n",
    "    if topk is not None:\n",
    "        # return top-k labels + probs as a small list for analysis\n",
    "        topk_idx = probs.argsort()[::-1][:topk]\n",
    "        topk_labels = [idx_to_group[i] for i in topk_idx]\n",
    "        topk_probs = probs[topk_idx]\n",
    "        return pred_group, probs, topk_labels, topk_probs\n",
    "\n",
    "    return pred_group, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121c522-82af-42ef-abad-88df07f68f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/alligator2_original_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7ac38-c577-413d-9f5f-85eb26b95c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/alligator2_1_8_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4de510-8fc5-48c4-8d82-0af2c19c4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/beetle_original.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9922362-600e-4ad4-92a5-014e2ee19ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/beetle_0_10.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05651970-8ca8-471c-ab61-f2559b275715",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/beetle_10_15.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70667054-df31-4e16-ab58-451ab7f27241",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/elephant7_original_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd194e5f-ba58-46ac-bd75-294cef3e6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/elephant_1_5_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b8485-923c-4c03-a959-f64a032db7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/hawk5_original_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275d087-6422-4a91-972e-cb40589a7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyNewImage('PCA_images/hawk5_1_5_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3abca4f-8ec5-4550-bb03-d651f040e63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>pred_group</th>\n",
       "      <th>confidence_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alligator2_original_.jpg</td>\n",
       "      <td>PCA_images/alligator2_original_.jpg</td>\n",
       "      <td>reptiles</td>\n",
       "      <td>85.019684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alligator2_1_8_.jpg</td>\n",
       "      <td>PCA_images/alligator2_1_8_.jpg</td>\n",
       "      <td>reptiles</td>\n",
       "      <td>68.069148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beetle_original.jpg</td>\n",
       "      <td>PCA_images/beetle_original.jpg</td>\n",
       "      <td>bugs</td>\n",
       "      <td>95.490265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beetle_0_10.jpg</td>\n",
       "      <td>PCA_images/beetle_0_10.jpg</td>\n",
       "      <td>bugs</td>\n",
       "      <td>84.802097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beetle_10_15.jpg</td>\n",
       "      <td>PCA_images/beetle_10_15.jpg</td>\n",
       "      <td>bugs</td>\n",
       "      <td>61.293793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elephant7_original_.jpg</td>\n",
       "      <td>PCA_images/elephant7_original_.jpg</td>\n",
       "      <td>mammals</td>\n",
       "      <td>95.410377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>elephant_1_5_.jpg</td>\n",
       "      <td>PCA_images/elephant_1_5_.jpg</td>\n",
       "      <td>mammals</td>\n",
       "      <td>93.283194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hawk5_original_.jpg</td>\n",
       "      <td>PCA_images/hawk5_original_.jpg</td>\n",
       "      <td>birds</td>\n",
       "      <td>95.783722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hawk5_1_5_.jpg</td>\n",
       "      <td>PCA_images/hawk5_1_5_.jpg</td>\n",
       "      <td>birds</td>\n",
       "      <td>90.796012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image                                 path pred_group  \\\n",
       "0  alligator2_original_.jpg  PCA_images/alligator2_original_.jpg   reptiles   \n",
       "1       alligator2_1_8_.jpg       PCA_images/alligator2_1_8_.jpg   reptiles   \n",
       "2       beetle_original.jpg       PCA_images/beetle_original.jpg       bugs   \n",
       "3           beetle_0_10.jpg           PCA_images/beetle_0_10.jpg       bugs   \n",
       "4          beetle_10_15.jpg          PCA_images/beetle_10_15.jpg       bugs   \n",
       "5   elephant7_original_.jpg   PCA_images/elephant7_original_.jpg    mammals   \n",
       "6         elephant_1_5_.jpg         PCA_images/elephant_1_5_.jpg    mammals   \n",
       "7       hawk5_original_.jpg       PCA_images/hawk5_original_.jpg      birds   \n",
       "8            hawk5_1_5_.jpg            PCA_images/hawk5_1_5_.jpg      birds   \n",
       "\n",
       "   confidence_pct  \n",
       "0       85.019684  \n",
       "1       68.069148  \n",
       "2       95.490265  \n",
       "3       84.802097  \n",
       "4       61.293793  \n",
       "5       95.410377  \n",
       "6       93.283194  \n",
       "7       95.783722  \n",
       "8       90.796012  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results= pd.DataFrame(classificationResults)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907a7e2-02b9-4d69-b378-0ea33600df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
